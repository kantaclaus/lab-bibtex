


@article{kay_arXiv2017_kinetics400,
  author        = {Will Kay and Jo{\~{a}}o Carreira and Karen Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and Trevor Back and Paul Natsev and Mustafa Suleyman and Andrew Zisserman},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/KayCSZHVVGBNSZ17.bib},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  eprint        = {1705.06950},
  eprinttype    = {arXiv},
  journal       = {CoRR},
  timestamp     = {Thu, 14 Oct 2021 09:15:04 +0200},
  title         = {The Kinetics Human Action Video Dataset},
  url           = {http://arxiv.org/abs/1705.06950},
  volume        = {abs/1705.06950},
  year          = {2017},
  bdsk-url-1    = {http://arxiv.org/abs/1705.06950}
}

@article{Carreira_arXiv2018_kinetics600,
  author        = {Jo{\~{a}}o Carreira and Eric Noland and Andras Banki{-}Horvath and Chloe Hillier and Andrew Zisserman},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1808-01340.bib},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  eprint        = {1808.01340},
  eprinttype    = {arXiv},
  journal       = {CoRR},
  timestamp     = {Sun, 02 Sep 2018 15:01:55 +0200},
  title         = {A Short Note about Kinetics-600},
  url           = {http://arxiv.org/abs/1808.01340},
  volume        = {abs/1808.01340},
  year          = {2018},
  bdsk-url-1    = {http://arxiv.org/abs/1808.01340}
}

@article{Carreira_arXiv2019_kinetics700,
  author        = {Jo{\~{a}}o Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1907-06987.bib},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  eprint        = {1907.06987},
  eprinttype    = {arXiv},
  journal       = {CoRR},
  timestamp     = {Tue, 23 Jul 2019 10:54:22 +0200},
  title         = {A Short Note on the Kinetics-700 Human Action Dataset},
  url           = {http://arxiv.org/abs/1907.06987},
  volume        = {abs/1907.06987},
  year          = {2019},
  bdsk-url-1    = {http://arxiv.org/abs/1907.06987}
}

@article{Smaira_arXiv2020_kinetics700_2020,
  author        = {Lucas Smaira and Jo{\~{a}}o Carreira and Eric Noland and Ellen Clancy and Amy Wu and Andrew Zisserman},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2010-10864.bib},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  eprint        = {2010.10864},
  eprinttype    = {arXiv},
  journal       = {CoRR},
  timestamp     = {Mon, 26 Oct 2020 15:39:44 +0100},
  title         = {A Short Note on the Kinetics-700-2020 Human Action Dataset},
  url           = {https://arxiv.org/abs/2010.10864},
  volume        = {abs/2010.10864},
  year          = {2020},
  bdsk-url-1    = {https://arxiv.org/abs/2010.10864}
}

@article{Soomro_arXiv2012_UCF101,
  author        = {Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1212-0402.bib},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  eprint        = {1212.0402},
  eprinttype    = {arXiv},
  journal       = {CoRR},
  timestamp     = {Mon, 13 Aug 2018 16:47:45 +0200},
  title         = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in The Wild},
  url           = {http://arxiv.org/abs/1212.0402},
  volume        = {abs/1212.0402},
  year          = {2012},
  bdsk-url-1    = {http://arxiv.org/abs/1212.0402}
}

@inproceedings{Kuehne_ICCV2011_HMDB51,
  author        = {Hildegard Kuehne and Hueihan Jhuang and Est{\'{\i}}baliz Garrote and Tomaso A. Poggio and Thomas Serre},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/conf/iccv/KuehneJGPS11.bib},
  booktitle     = {{IEEE} International Conference on Computer Vision, {ICCV} 2011, Barcelona, Spain, November 6-13, 2011},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  doi           = {10.1109/ICCV.2011.6126543},
  editor        = {Dimitris N. Metaxas and Long Quan and Alberto Sanfeliu and Luc Van Gool},
  pages         = {2556--2563},
  publisher     = {{IEEE} Computer Society},
  timestamp     = {Thu, 14 Oct 2021 10:42:45 +0200},
  title         = {{HMDB:} {A} large video database for human motion recognition},
  url           = {https://doi.org/10.1109/ICCV.2011.6126543},
  year          = {2011},
  bdsk-url-1    = {https://doi.org/10.1109/ICCV.2011.6126543}
}

@inproceedings{Jhuang_ICCV2013_JHMDB21,
  author    = {Hueihan Jhuang and
               Juergen Gall and
               Silvia Zuffi and
               Cordelia Schmid and
               Michael J. Black},
  title     = {Towards Understanding Action Recognition},
  booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2013, Sydney,
               Australia, December 1-8, 2013},
  pages     = {3192--3199},
  publisher = {{IEEE} Computer Society},
  year      = {2013},
  url       = {https://doi.org/10.1109/ICCV.2013.396},
  doi       = {10.1109/ICCV.2013.396},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/JhuangGZSB13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Goyal_2017ICCV_ssv2,
  author        = {Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and Hoppe, Florian and Thurau, Christian and Bax, Ingo and Memisevic, Roland},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  month         = {Oct},
  title         = {The "Something Something" Video Database for Learning and Evaluating Visual Common Sense},
  year          = {2017}
}



@inproceedings{Heilbron_2015CVPR_ActivityNet,
  author    = {Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  title     = {ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2015}
}

@inproceedings{Krishna_2017ICCV_ActivityNet-Captions,
  author    = {Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  title     = {Dense-Captioning Events in Videos},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}

@inproceedings{Zhou_2019CVPR_ActivityNet-Entities,
  author    = {Zhou, Luowei and Kalantidis, Yannis and Chen, Xinlei and Corso, Jason J. and Rohrbach, Marcus},
  title     = {Grounded Video Description},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}

@inproceedings{damen_ECCV2018_Epic-Kitchens55,
  author        = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael},
  booktitle     = {Proceedings of the European Conference on Computer Vision (ECCV)},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  month         = {September},
  pages         = {720--736},
  title         = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},
  year          = {2018}
}

@article{Damen_IJCV2022_EPIC-KITCHENS-100,
  author        = {Dima Damen and Hazel Doughty and Giovanni Maria Farinella and Antonino Furnari and Evangelos Kazakos and Jian Ma and Davide Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/ijcv/DamenDFFKMMMPPW22.bib},
  date-added    = {2022-10-21 08:27:16 +0900},
  date-modified = {2022-10-21 08:27:16 +0900},
  doi           = {10.1007/s11263-021-01531-2},
  journal       = {Int. J. Comput. Vis.},
  number        = {1},
  pages         = {33--55},
  timestamp     = {Sat, 09 Apr 2022 12:32:32 +0200},
  title         = {Rescaling Egocentric Vision: Collection, Pipeline and Challenges for {EPIC-KITCHENS-100}},
  url           = {https://doi.org/10.1007/s11263-021-01531-2},
  volume        = {130},
  year          = {2022},
  bdsk-url-1    = {https://doi.org/10.1007/s11263-021-01531-2}
}

@article{Idrees_CVIU2017_THUMOS,
  title    = {The THUMOS challenge on action recognition for videos “in the wild”},
  journal  = {Computer Vision and Image Understanding},
  volume   = {155},
  pages    = {1-23},
  year     = {2017},
  issn     = {1077-3142},
  doi      = {https://doi.org/10.1016/j.cviu.2016.10.018},
  url      = {https://www.sciencedirect.com/science/article/pii/S1077314216301710},
  author   = {Haroon Idrees and Amir R. Zamir and Yu-Gang Jiang and Alex Gorban and Ivan Laptev and Rahul Sukthankar and Mubarak Shah},
  keywords = {Action recognition, Action detection, Action localization, Untrimmed videos, THUMOS, Dataset, Benchmark, UCF101}
}

@inproceedings{Gu_2018CVPR_AVA-Actions,
  author    = {Gu, Chunhui and Sun, Chen and Ross, David A. and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and Schmid, Cordelia and Malik, Jitendra},
  title     = {AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2018}
}

@article{Li_arXiv2020_AVA-Kinetics,
  author     = {Ang Li and
                Meghana Thotakuri and
                David A. Ross and
                Jo{\~{a}}o Carreira and
                Alexander Vostrikov and
                Andrew Zisserman},
  title      = {The AVA-Kinetics Localized Human Actions Video Dataset},
  journal    = {CoRR},
  volume     = {abs/2005.00214},
  year       = {2020},
  url        = {https://arxiv.org/abs/2005.00214},
  eprinttype = {arXiv},
  eprint     = {2005.00214},
  timestamp  = {Fri, 08 May 2020 15:04:04 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2005-00214.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Yuval_NIPS2011_SVHN,
  author        = {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
  booktitle     = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011},
  date-added    = {2022-10-21 08:25:31 +0900},
  date-modified = {2022-10-21 08:25:31 +0900},
  title         = {Reading Digits in Natural Images with Unsupervised Feature Learning},
  url           = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
  year          = {2011},
  bdsk-url-1    = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf}
}

@inproceedings{Lin_ECCV2014_COCO,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{\'{a}}r and
               C. Lawrence Zitnick},
  editor    = {David J. Fleet and
               Tom{\'{a}}s Pajdla and
               Bernt Schiele and
               Tinne Tuytelaars},
  title     = {Microsoft {COCO:} Common Objects in Context},
  booktitle = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
               Switzerland, September 6-12, 2014, Proceedings, Part {V}},
  series    = {Lecture Notes in Computer Science},
  volume    = {8693},
  pages     = {740--755},
  publisher = {Springer},
  year      = {2014},
  url       = {https://doi.org/10.1007/978-3-319-10602-1\_48},
  doi       = {10.1007/978-3-319-10602-1\_48},
  timestamp = {Thu, 23 Jun 2022 19:55:44 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/LinMBHPRDZ14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Feichtenhofer_2020CVPR_X3D,
  author    = {Feichtenhofer, Christoph},
  title     = {X3D: Expanding Architectures for Efficient Video Recognition},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2020}
}

@inproceedings{Feichtenhofer_2019ICCV_SlowFast,
  author    = {Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  title     = {SlowFast Networks for Video Recognition},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2019}
}

@inproceedings{Hara_2018CVPR_3D_ResNet,
  author    = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  title     = {Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2018}
}

@inproceedings{Lin_2019ICCV_TSM,
  author    = {Lin, Ji and Gan, Chuang and Han, Song},
  title     = {TSM: Temporal Shift Module for Efficient Video Understanding},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2019}
}

@inproceedings{Zhang_ACMMM2021_TokenShift,
  author    = {Zhang, Hao and Hao, Yanbin and Ngo, Chong-Wah},
  title     = {Token Shift Transformer for Video Classification},
  year      = {2021},
  isbn      = {9781450386517},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3474085.3475272},
  doi       = {10.1145/3474085.3475272},
  abstract  = {Transformer achieves remarkable successes in understanding 1 and 2-dimensional signals (e.g., NLP and Image Content Understanding). As a potential alternative to convolutional neural networks, it shares merits of strong interpretability, high discriminative power on hyper-scale data, and flexibility in processing varying length inputs. However, its encoders naturally contain computational intensive operations such as pair-wise self-attention, incurring heavy computational burden when being applied on the complex 3-dimensional video signals. This paper presents Token Shift Module (i.e., TokShift), a novel, zero-parameter, zero-FLOPs operator, for modeling temporal relations within each transformer encoder. Specifically, the TokShift barely temporally shifts partial [Class] token features back-and-forth across adjacent frames. Then, we densely plug the module into each encoder of a plain 2D vision transformer for learning 3D video representation. It is worth noticing that our TokShift transformer is a pure convolutional-free video transformer pilot with computational efficiency for video understanding. Experiments on standard benchmarks verify its robustness, effectiveness, and efficiency. Particularly, with input clips of 8/12 frames, the TokShift transformer achieves SOTA precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80% on UCF-101 datasets, comparable or better than existing SOTA convolutional counterparts. Our code is open-sourced in: https://github.com/VideoNetworks/TokShift-Transformer.},
  booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages     = {917–925},
  numpages  = {9},
  keywords  = {transformer, self-attention, shift, video classification},
  location  = {Virtual Event, China},
  series    = {MM '21}
}

@inproceedings{Arnab_2021_ICCV_ViVit,
  author    = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu\v{c}i\'c, Mario and Schmid, Cordelia},
  title     = {ViViT: A Video Vision Transformer},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {6836-6846}
}

@inproceedings{Bertasius_ICML2021_TimeSformer,
  title     = {Is Space-Time Attention All You Need for Video Understanding?},
  author    = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {813--824},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  month     = {18--24 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v139/bertasius21a/bertasius21a.pdf},
  url       = {https://proceedings.mlr.press/v139/bertasius21a.html},
  abstract  = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named “TimeSformer,” adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that “divided attention,” where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.}
}

@article{Sharir_arxiv2021_STAM,
  author     = {Gilad Sharir and
                Asaf Noy and
                Lihi Zelnik{-}Manor},
  title      = {An Image is Worth 16x16 Words, What is a Video Worth?},
  journal    = {CoRR},
  volume     = {abs/2103.13915},
  year       = {2021},
  url        = {https://arxiv.org/abs/2103.13915},
  eprinttype = {arXiv},
  eprint     = {2103.13915},
  timestamp  = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2103-13915.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Neimark_2021_ICCVW_VideoTransformerNetwork,
  author    = {Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
  title     = {Video Transformer Network},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
  month     = {October},
  year      = {2021},
  pages     = {3163-3172}
}

@inproceedings{Girdhar_2019_CVPR_VideoActionTransformerNetwork,
  author    = {Girdhar, Rohit and Carreira, Joao and Doersch, Carl and Zisserman, Andrew},
  title     = {Video Action Transformer Network},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}